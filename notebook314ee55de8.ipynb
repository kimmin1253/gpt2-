{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:12.478968Z","iopub.execute_input":"2025-01-23T05:03:12.479258Z","iopub.status.idle":"2025-01-23T05:03:12.791760Z","shell.execute_reply.started":"2025-01-23T05:03:12.479236Z","shell.execute_reply":"2025-01-23T05:03:12.790865Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# 데이터 로드를 위함\nfrom datasets import load_dataset\n\n# 기본 파이썬 패키지\nimport pandas as pd\nimport numpy as np\nimport datetime\nfrom tqdm import tqdm\n\n# GPT 사용을 위함\nimport torch\nfrom transformers import BertTokenizer\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# for padding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# 전처리 및 평가 지표\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:12.792937Z","iopub.execute_input":"2025-01-23T05:03:12.793353Z","iopub.status.idle":"2025-01-23T05:03:37.232440Z","shell.execute_reply.started":"2025-01-23T05:03:12.793319Z","shell.execute_reply":"2025-01-23T05:03:37.231769Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/ukairia777/finance_sentiment_corpus/main/finance_data.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:37.233907Z","iopub.execute_input":"2025-01-23T05:03:37.234408Z","iopub.status.idle":"2025-01-23T05:03:37.924171Z","shell.execute_reply.started":"2025-01-23T05:03:37.234383Z","shell.execute_reply":"2025-01-23T05:03:37.923318Z"}},"outputs":[{"name":"stdout","text":"--2025-01-23 05:03:37--  https://raw.githubusercontent.com/ukairia777/finance_sentiment_corpus/main/finance_data.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1319001 (1.3M) [text/plain]\nSaving to: ‘finance_data.csv’\n\nfinance_data.csv    100%[===================>]   1.26M  --.-KB/s    in 0.05s   \n\n2025-01-23 05:03:37 (23.9 MB/s) - ‘finance_data.csv’ saved [1319001/1319001]\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"df = pd.read_csv('finance_data.csv')\nprint('샘플의 개수 :', len(df))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:37.925618Z","iopub.execute_input":"2025-01-23T05:03:37.925839Z","iopub.status.idle":"2025-01-23T05:03:37.971109Z","shell.execute_reply.started":"2025-01-23T05:03:37.925818Z","shell.execute_reply":"2025-01-23T05:03:37.970278Z"}},"outputs":[{"name":"stdout","text":"샘플의 개수 : 4846\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:37.972067Z","iopub.execute_input":"2025-01-23T05:03:37.972405Z","iopub.status.idle":"2025-01-23T05:03:38.000869Z","shell.execute_reply.started":"2025-01-23T05:03:37.972373Z","shell.execute_reply":"2025-01-23T05:03:37.999950Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"     labels                                           sentence  \\\n0   neutral  According to Gran, the company has no plans to...   \n1   neutral  Technopolis plans to develop in stages an area...   \n2  negative  The international electronic industry company ...   \n3  positive  With the new production plant the company woul...   \n4  positive  According to the company's updated strategy fo...   \n\n                                        kor_sentence  \n0  Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...  \n1  테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...  \n2  국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...  \n3  새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...  \n4  2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>sentence</th>\n      <th>kor_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neutral</td>\n      <td>According to Gran, the company has no plans to...</td>\n      <td>Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neutral</td>\n      <td>Technopolis plans to develop in stages an area...</td>\n      <td>테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>negative</td>\n      <td>The international electronic industry company ...</td>\n      <td>국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>positive</td>\n      <td>With the new production plant the company woul...</td>\n      <td>새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>positive</td>\n      <td>According to the company's updated strategy fo...</td>\n      <td>2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"df['labels'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.001852Z","iopub.execute_input":"2025-01-23T05:03:38.002185Z","iopub.status.idle":"2025-01-23T05:03:38.014205Z","shell.execute_reply.started":"2025-01-23T05:03:38.002151Z","shell.execute_reply":"2025-01-23T05:03:38.013316Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"labels\nneutral     2879\npositive    1363\nnegative     604\nName: count, dtype: int64"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"df['labels'] = df['labels'].replace(['neutral', 'positive', 'negative'],[0, 1, 2])\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.015104Z","iopub.execute_input":"2025-01-23T05:03:38.015441Z","iopub.status.idle":"2025-01-23T05:03:38.035454Z","shell.execute_reply.started":"2025-01-23T05:03:38.015407Z","shell.execute_reply":"2025-01-23T05:03:38.034663Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-8-910beaa298a3>:1: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df['labels'] = df['labels'].replace(['neutral', 'positive', 'negative'],[0, 1, 2])\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"   labels                                           sentence  \\\n0       0  According to Gran, the company has no plans to...   \n1       0  Technopolis plans to develop in stages an area...   \n2       2  The international electronic industry company ...   \n3       1  With the new production plant the company woul...   \n4       1  According to the company's updated strategy fo...   \n\n                                        kor_sentence  \n0  Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...  \n1  테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...  \n2  국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...  \n3  새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...  \n4  2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>labels</th>\n      <th>sentence</th>\n      <th>kor_sentence</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>According to Gran, the company has no plans to...</td>\n      <td>Gran에 따르면, 그 회사는 회사가 성장하고 있는 곳이지만, 모든 생산을 러시아로...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>Technopolis plans to develop in stages an area...</td>\n      <td>테크노폴리스는 컴퓨터 기술과 통신 분야에서 일하는 회사들을 유치하기 위해 10만 평...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>The international electronic industry company ...</td>\n      <td>국제 전자산업 회사인 엘코텍은 탈린 공장에서 수십 명의 직원을 해고했으며, 이전의 ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>With the new production plant the company woul...</td>\n      <td>새로운 생산공장으로 인해 회사는 예상되는 수요 증가를 충족시킬 수 있는 능력을 증가...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>According to the company's updated strategy fo...</td>\n      <td>2009-2012년 회사의 업데이트된 전략에 따르면, Basware는 20% - 4...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df.to_csv('finance_data.csv', index=False, encoding='utf-8-sig')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.037967Z","iopub.execute_input":"2025-01-23T05:03:38.038190Z","iopub.status.idle":"2025-01-23T05:03:38.076761Z","shell.execute_reply.started":"2025-01-23T05:03:38.038171Z","shell.execute_reply":"2025-01-23T05:03:38.076108Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"all_data = load_dataset(\n        \"csv\",\n        data_files={\n            \"train\": \"finance_data.csv\",\n        },\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.078119Z","iopub.execute_input":"2025-01-23T05:03:38.078377Z","iopub.status.idle":"2025-01-23T05:03:38.303887Z","shell.execute_reply.started":"2025-01-23T05:03:38.078357Z","shell.execute_reply":"2025-01-23T05:03:38.303043Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f81d0d521c44856a1964179bdf27833"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"cs = all_data['train'].train_test_split(0.2, seed=777)\ntrain_cs = cs[\"train\"]\ntest_cs = cs[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.304756Z","iopub.execute_input":"2025-01-23T05:03:38.305025Z","iopub.status.idle":"2025-01-23T05:03:38.317027Z","shell.execute_reply.started":"2025-01-23T05:03:38.305004Z","shell.execute_reply":"2025-01-23T05:03:38.316372Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"train_cs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.317761Z","iopub.execute_input":"2025-01-23T05:03:38.317950Z","iopub.status.idle":"2025-01-23T05:03:38.332308Z","shell.execute_reply.started":"2025-01-23T05:03:38.317933Z","shell.execute_reply":"2025-01-23T05:03:38.331399Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'sentence', 'kor_sentence'],\n    num_rows: 3876\n})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"test_cs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.333322Z","iopub.execute_input":"2025-01-23T05:03:38.333574Z","iopub.status.idle":"2025-01-23T05:03:38.350262Z","shell.execute_reply.started":"2025-01-23T05:03:38.333542Z","shell.execute_reply":"2025-01-23T05:03:38.348573Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'sentence', 'kor_sentence'],\n    num_rows: 970\n})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# 훈련 데이터를 다시 8:2로 분리 후 훈련 데이터와 검증 데이터로 저장\ncs = train_cs.train_test_split(0.2, seed=777)\ntrain_cs = cs[\"train\"]\nvalid_cs = cs[\"test\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.351163Z","iopub.execute_input":"2025-01-23T05:03:38.351428Z","iopub.status.idle":"2025-01-23T05:03:38.379360Z","shell.execute_reply.started":"2025-01-23T05:03:38.351399Z","shell.execute_reply":"2025-01-23T05:03:38.378570Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_cs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.380390Z","iopub.execute_input":"2025-01-23T05:03:38.380738Z","iopub.status.idle":"2025-01-23T05:03:38.386234Z","shell.execute_reply.started":"2025-01-23T05:03:38.380706Z","shell.execute_reply":"2025-01-23T05:03:38.385252Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'sentence', 'kor_sentence'],\n    num_rows: 3100\n})"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"valid_cs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.387208Z","iopub.execute_input":"2025-01-23T05:03:38.387561Z","iopub.status.idle":"2025-01-23T05:03:38.400386Z","shell.execute_reply.started":"2025-01-23T05:03:38.387527Z","shell.execute_reply":"2025-01-23T05:03:38.399561Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'sentence', 'kor_sentence'],\n    num_rows: 776\n})"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"test_cs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.401354Z","iopub.execute_input":"2025-01-23T05:03:38.401665Z","iopub.status.idle":"2025-01-23T05:03:38.417344Z","shell.execute_reply.started":"2025-01-23T05:03:38.401637Z","shell.execute_reply":"2025-01-23T05:03:38.416558Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['labels', 'sentence', 'kor_sentence'],\n    num_rows: 970\n})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"print('두번째 샘플 출력 :', train_cs['kor_sentence'][1])\nprint('두번째 샘플의 레이블 출력 :', train_cs['labels'][1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.418326Z","iopub.execute_input":"2025-01-23T05:03:38.418676Z","iopub.status.idle":"2025-01-23T05:03:38.467016Z","shell.execute_reply.started":"2025-01-23T05:03:38.418646Z","shell.execute_reply":"2025-01-23T05:03:38.462172Z"}},"outputs":[{"name":"stdout","text":"두번째 샘플 출력 : 알마 미디어 코퍼레이션 사업 ID 1944757-4의 주식 자본금은 44,767,513.80유로이며 74,612,523주로 나뉜다.\n두번째 샘플의 레이블 출력 : 0\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# 훈련 데이터, 검증 데이터, 테스트 데이터\ntrain_sentences = list(train_cs['kor_sentence'])\nvalidation_sentences = list(valid_cs['kor_sentence'])\ntest_sentences = list(test_cs['kor_sentence'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.468032Z","iopub.execute_input":"2025-01-23T05:03:38.468313Z","iopub.status.idle":"2025-01-23T05:03:38.497345Z","shell.execute_reply.started":"2025-01-23T05:03:38.468291Z","shell.execute_reply":"2025-01-23T05:03:38.496543Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"train_labels = train_cs['labels']\nvalidation_labels = valid_cs['labels']\ntest_labels = test_cs['labels']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.498196Z","iopub.execute_input":"2025-01-23T05:03:38.498473Z","iopub.status.idle":"2025-01-23T05:03:38.523671Z","shell.execute_reply.started":"2025-01-23T05:03:38.498447Z","shell.execute_reply":"2025-01-23T05:03:38.522978Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"test_sentences[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.524844Z","iopub.execute_input":"2025-01-23T05:03:38.525164Z","iopub.status.idle":"2025-01-23T05:03:38.531031Z","shell.execute_reply.started":"2025-01-23T05:03:38.525134Z","shell.execute_reply":"2025-01-23T05:03:38.530169Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"['오전 10.58시 아우토쿰푸는 2.74pct 하락한 24.87유로, OMX 헬싱키 25지수는 0.55pct 상승한 2,825.14, OMX 헬싱키는 0.64pct 하락한 9,386.89유로에 거래됐다.',\n '10월부터 12월까지의 판매량은 302 mln 유로로 전년 동기 대비 25.3 pct 증가했다.',\n '매디슨, 위스콘신, 2월 6일 - PRNewswire - - 피스카스는 미국 특허청이 상징적인 가위 손잡이에 오렌지색 상표 등록을 허가했다고 발표한다.',\n \"M-real로 평가된 분석가들 중 총 6명은 ''매수' - ''누적''을 주었고, 3명은 ''보유'', 1명만이 ''매도''를 주었다.\",\n '주요 양조업체들은 지난해 국내 맥주 판매량을 2004년 2억4592만 리터에서 2억5688만 리터로 4.5% 늘렸다.']"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"test_sentences[:5]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.531841Z","iopub.execute_input":"2025-01-23T05:03:38.532023Z","iopub.status.idle":"2025-01-23T05:03:38.545406Z","shell.execute_reply.started":"2025-01-23T05:03:38.532006Z","shell.execute_reply":"2025-01-23T05:03:38.544585Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"['오전 10.58시 아우토쿰푸는 2.74pct 하락한 24.87유로, OMX 헬싱키 25지수는 0.55pct 상승한 2,825.14, OMX 헬싱키는 0.64pct 하락한 9,386.89유로에 거래됐다.',\n '10월부터 12월까지의 판매량은 302 mln 유로로 전년 동기 대비 25.3 pct 증가했다.',\n '매디슨, 위스콘신, 2월 6일 - PRNewswire - - 피스카스는 미국 특허청이 상징적인 가위 손잡이에 오렌지색 상표 등록을 허가했다고 발표한다.',\n \"M-real로 평가된 분석가들 중 총 6명은 ''매수' - ''누적''을 주었고, 3명은 ''보유'', 1명만이 ''매도''를 주었다.\",\n '주요 양조업체들은 지난해 국내 맥주 판매량을 2004년 2억4592만 리터에서 2억5688만 리터로 4.5% 늘렸다.']"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:38.546322Z","iopub.execute_input":"2025-01-23T05:03:38.546638Z","iopub.status.idle":"2025-01-23T05:03:43.257008Z","shell.execute_reply.started":"2025-01-23T05:03:38.546611Z","shell.execute_reply":"2025-01-23T05:03:43.256007Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:43.260893Z","iopub.execute_input":"2025-01-23T05:03:43.261137Z","iopub.status.idle":"2025-01-23T05:03:43.299126Z","shell.execute_reply.started":"2025-01-23T05:03:43.261114Z","shell.execute_reply":"2025-01-23T05:03:43.298557Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"# 한국어 GPT 중 하나인 'skt/kogpt2-base-v2'를 사용.\ntokenizer = AutoTokenizer.from_pretrained('skt/kogpt2-base-v2')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:43.300136Z","iopub.execute_input":"2025-01-23T05:03:43.300326Z","iopub.status.idle":"2025-01-23T05:03:44.257703Z","shell.execute_reply.started":"2025-01-23T05:03:43.300309Z","shell.execute_reply":"2025-01-23T05:03:44.256992Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e82f6380c4734822926886e8b5a48c0a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ef9ecee93494c9181071502ac4998bd"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"# 최대 길이는 128\nMAX_LEN = 128\ndef data_to_tensor (sentences, labels, MAX_LEN):\n  # 정수 인코딩 과정. 각 텍스트를 토큰화한 후에 Vocabulary에 맵핑되는 정수 시퀀스로 변환한다.\n  # ex) ['안녕하세요'] ==> ['안', '녕', '하세요'] ==> [231, 52, 45]\n  tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n  input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n\n  # pad_sequences는 패딩을 위한 모듈. 주어진 최대 길이를 위해서 뒤에서 패딩 토큰의 번호로 채워준다.\n  # ex) [231, 52, 45] ==> [231, 52, 45, 패딩 토큰, 패딩 토큰, 패딩 토큰]\n  pad_token = tokenizer.encode('<pad>')[0]\n  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, value=pad_token, dtype=\"long\", truncating=\"post\", padding=\"post\") \n\n  attention_masks = []\n\n  for seq in input_ids:\n      seq_mask = [float(i != pad_token) for i in seq]\n      attention_masks.append(seq_mask)\n\n  tensor_inputs = torch.tensor(input_ids)\n  tensor_labels = torch.tensor(labels)\n  tensor_masks = torch.tensor(attention_masks)\n\n  return tensor_inputs, tensor_labels, tensor_masks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:44.258520Z","iopub.execute_input":"2025-01-23T05:03:44.258774Z","iopub.status.idle":"2025-01-23T05:03:44.264033Z","shell.execute_reply.started":"2025-01-23T05:03:44.258743Z","shell.execute_reply":"2025-01-23T05:03:44.263374Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# 학습 데이터, 검증 데이터, 테스트 데이터에 대해서\n# 정수 인코딩 결과, 레이블, 어텐션 마스크를 각각 inputs, labels, masks에 저장.\ntrain_inputs, train_labels, train_masks = data_to_tensor(train_sentences, train_labels, MAX_LEN)\nvalidation_inputs, validation_labels, validation_masks = data_to_tensor(validation_sentences, validation_labels, MAX_LEN)\ntest_inputs, test_labels, test_masks = data_to_tensor(test_sentences, test_labels, MAX_LEN)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:44.264941Z","iopub.execute_input":"2025-01-23T05:03:44.265195Z","iopub.status.idle":"2025-01-23T05:03:45.184171Z","shell.execute_reply.started":"2025-01-23T05:03:44.265175Z","shell.execute_reply":"2025-01-23T05:03:45.183503Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"batch_size = 32\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:45.184939Z","iopub.execute_input":"2025-01-23T05:03:45.185257Z","iopub.status.idle":"2025-01-23T05:03:45.192168Z","shell.execute_reply.started":"2025-01-23T05:03:45.185225Z","shell.execute_reply":"2025-01-23T05:03:45.191314Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:45.193101Z","iopub.execute_input":"2025-01-23T05:03:45.193429Z","iopub.status.idle":"2025-01-23T05:03:45.206572Z","shell.execute_reply.started":"2025-01-23T05:03:45.193398Z","shell.execute_reply":"2025-01-23T05:03:45.205942Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"test_data = TensorDataset(test_inputs, test_masks, test_labels)\ntest_sampler = RandomSampler(test_data)\ntest_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:45.207406Z","iopub.execute_input":"2025-01-23T05:03:45.207705Z","iopub.status.idle":"2025-01-23T05:03:45.233070Z","shell.execute_reply.started":"2025-01-23T05:03:45.207676Z","shell.execute_reply":"2025-01-23T05:03:45.232255Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\nelse:\n    device = torch.device(\"cpu\")\n    print('No GPU available, using the CPU instead.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:45.233855Z","iopub.execute_input":"2025-01-23T05:03:45.234113Z","iopub.status.idle":"2025-01-23T05:03:45.382172Z","shell.execute_reply.started":"2025-01-23T05:03:45.234086Z","shell.execute_reply":"2025-01-23T05:03:45.381276Z"}},"outputs":[{"name":"stdout","text":"There are 2 GPU(s) available.\nWe will use the GPU: Tesla T4\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"num_labels = 3\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\"skt/kogpt2-base-v2\", num_labels=num_labels)\nmodel.cuda()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:45.383170Z","iopub.execute_input":"2025-01-23T05:03:45.383580Z","iopub.status.idle":"2025-01-23T05:03:52.295221Z","shell.execute_reply.started":"2025-01-23T05:03:45.383552Z","shell.execute_reply":"2025-01-23T05:03:52.294278Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3db88ce90224147aacfa3d00653eed5"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"GPT2ForSequenceClassification(\n  (transformer): GPT2Model(\n    (wte): Embedding(51200, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2SdpaAttention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (score): Linear(in_features=768, out_features=3, bias=False)\n)"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"# 몇 번의 에포크(전체 데이터에 대한 학습 횟수)를 할 것인지 선택\nepochs = 3\n\n# 옵티마이저 선택\noptimizer = AdamW(model.parameters(), lr = 2e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:52.296644Z","iopub.execute_input":"2025-01-23T05:03:52.296953Z","iopub.status.idle":"2025-01-23T05:03:52.306633Z","shell.execute_reply.started":"2025-01-23T05:03:52.296905Z","shell.execute_reply":"2025-01-23T05:03:52.305689Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"def metrics(predictions, labels):\n    # predictions: 모델이 예측한 결과값들의 리스트 또는 배열\n    # labels: 실제 정답 레이블들의 리스트 또는 배열\n\n    # 예측값과 실제 레이블을 별도의 변수에 할당\n    y_pred = predictions\n    y_true = labels\n\n    # 사용 가능한 메트릭들을 계산\n\n    # 정확도 (Accuracy)\n    # 전체 예측 중에서 올바르게 예측한 비율\n    accuracy = accuracy_score(y_true, y_pred)\n\n    # 매크로 평균 F1 점수 (Macro-averaged F1 Score)\n    # 클래스별로 F1 점수를 계산한 후, 그 평균을 구함\n    # zero_division=0 옵션은 분모가 0일 경우 0을 반환하도록 설정\n    f1_macro_average = f1_score(y_true=y_true, y_pred=y_pred, average='macro', zero_division=0)\n\n    # 마이크로 평균 F1 점수 (Micro-averaged F1 Score)\n    # 전체 데이터에 대해 단일 F1 점수를 계산\n    # 클래스 불균형이 심한 경우에 적합\n    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro', zero_division=0)\n\n    # 가중 평균 F1 점수 (Weighted-averaged F1 Score)\n    # 각 클래스의 F1 점수에 해당 클래스의 샘플 수를 가중치로 곱한 후 평균을 구함\n    f1_weighted_average = f1_score(y_true=y_true, y_pred=y_pred, average='weighted', zero_division=0)\n\n    # 계산된 메트릭 결과를 딕셔너리 형태로 리턴\n    metrics = {'accuracy': accuracy,\n               'f1_macro': f1_macro_average,\n               'f1_micro': f1_micro_average,\n               'f1_weighted': f1_weighted_average}\n    return metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:52.307588Z","iopub.execute_input":"2025-01-23T05:03:52.307878Z","iopub.status.idle":"2025-01-23T05:03:52.317045Z","shell.execute_reply.started":"2025-01-23T05:03:52.307850Z","shell.execute_reply":"2025-01-23T05:03:52.316357Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def train_epoch(model, train_dataloader, optimizer, device):\n    \"\"\"\n    하나의 에포크 동안 모델을 학습시키는 함수입니다.\n\n    Parameters:\n    model (torch.nn.Module): 학습시킬 모델 객체.\n    train_dataloader (torch.utils.data.DataLoader): 학습 데이터셋의 DataLoader.\n    optimizer (torch.optim.Optimizer): 최적화 알고리즘을 구현하는 객체.\n    device (torch.device): 학습에 사용할 장치(CPU 또는 CUDA).\n\n    Returns:\n    float: 평균 학습 손실값.\n    \"\"\"\n\n    total_train_loss = 0  # 학습 손실을 누적할 변수 초기화\n    model.train()  # 모델을 학습 모드로 설정\n# 학습 데이터로더를 순회하며 배치 단위로 학습\n    for step, batch in tqdm(enumerate(train_dataloader), desc=\"Training Batch\"):\n        batch = tuple(t.to(device) for t in batch)  # DataLoader에서 배치를 받아 각 텐서를 지정된 장치로 이동\n        b_input_ids, b_input_mask, b_labels = batch  # 배치에서 입력 ID, 마스크, 라벨 추출\n\n        # 모델에 배치를 전달하여 손실값 계산\n        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n\n        # 손실값 추출\n        loss = outputs.loss\n\n        optimizer.zero_grad()  # 기울기(gradient) 초기화\n        loss.backward()  # 역전파를 통해 기울기(gradient) 계산\n        optimizer.step()  # 매개변수 업데이트\n\n        total_train_loss += loss.item()  # 총 손실에 더함\n\n    avg_train_loss = total_train_loss / len(train_dataloader)  # 평균 학습 손실 계산\n\n    return avg_train_loss  # 평균 학습 손실 반환","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:52.318007Z","iopub.execute_input":"2025-01-23T05:03:52.318218Z","iopub.status.idle":"2025-01-23T05:03:52.337274Z","shell.execute_reply.started":"2025-01-23T05:03:52.318192Z","shell.execute_reply":"2025-01-23T05:03:52.336469Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def evaluate(model, validation_dataloader, device):\n    \"\"\"\n    모델을 사용하여 검증 데이터셋에 대한 평가를 수행하는 함수입니다.\n\n    Parameters:\n    model (torch.nn.Module): 평가할 모델 객체.\n    validation_dataloader (torch.utils.data.DataLoader): 검증 데이터셋의 DataLoader.\n    device (torch.device): 평가에 사용할 장치(CPU 또는 CUDA).\n\n    Returns:\n    float: 평균 검증 손실값.\n    dict: 다양한 평가 지표(metrics)에 대한 값들을 담은 사전.\n    \"\"\"\n\n    model.eval()  # 모델을 평가 모드로 설정\n\n    total_eval_loss = 0  # 검증 손실을 누적할 변수 초기화\n    predictions, true_labels = [], []  # 예측값과 실제 라벨값을 저장할 리스트 초기화\n\n    # 검증 데이터로더를 순회하며 배치 단위로 평가\n    for batch in validation_dataloader:\n        batch = tuple(t.to(device) for t in batch)  # 배치 데이터를 디바이스로 이동\n        b_input_ids, b_input_mask, b_labels = batch  # 배치에서 입력 ID, 마스크, 라벨 추출\n\n        with torch.no_grad():  # 기울기(gradient) 계산을 수행하지 않음\n            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n\n        # 모델 출력에서 손실값 추출\n        if outputs.loss is not None:\n            loss = outputs.loss\n            total_eval_loss += loss.item()  # 총 손실에 더함\n\n        logits = outputs.logits.detach().cpu().numpy()  # 모델 예측값(로짓)을 numpy 배열로 변환\n        label_ids = b_labels.to('cpu').numpy()  # 실제 라벨값을 numpy 배열로 변환\n\n        # 3개의 값 중 가장 큰 값을 예측한 인덱스로 결정 (예시: logits = [3.513, -0.309, -2.111] ==> 예측: 0)\n        predictions.extend(np.argmax(logits, axis=1).flatten()) # 예측된 클래스를 리스트에 추가\n        true_labels.extend(label_ids.flatten()) # 실제 레이블 값을 리스트에 추가\n\n    eval_metrics = metrics(predictions, true_labels)\n\n    return total_eval_loss / len(validation_dataloader), eval_metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:52.338148Z","iopub.execute_input":"2025-01-23T05:03:52.338400Z","iopub.status.idle":"2025-01-23T05:03:52.355663Z","shell.execute_reply.started":"2025-01-23T05:03:52.338368Z","shell.execute_reply":"2025-01-23T05:03:52.354791Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# 최소 검증 손실 초기화\nmin_val_loss = float('inf')\n\n# 메인 학습 & 평가 루프\nfor epoch_i in range(0, epochs):\n    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n\n    # 학습 단계\n    train_epoch(model, train_dataloader, optimizer, device)\n\n    print(\"\\nRunning Validation...\")\n    # 검증 단계\n    avg_val_loss, eval_metrics = evaluate(model, validation_dataloader, device)\n    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n    print(\"  Accuracy: {0:.2f}\".format(eval_metrics['accuracy']))\n    print(\"  F1 Macro: {0:.2f}\".format(eval_metrics['f1_macro']))\n    print(\"  F1 Micro: {0:.2f}\".format(eval_metrics['f1_micro']))\n    print(\"  F1 Weighted: {0:.2f}\".format(eval_metrics['f1_weighted']))\n\n    # 검증 손실이 현재까지의 최소값보다 작은 경우 체크포인트 저장\n    if avg_val_loss < min_val_loss:\n        print(f\"Validation loss decreased ({min_val_loss:.2f} --> {avg_val_loss:.2f}).  Saving model ...\")\n        # 베스트 모델 저장\n        torch.save(model.state_dict(), 'model_checkpoint.pt')\n        # 최소 검증 손실 업데이트\n        min_val_loss = avg_val_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:03:52.356608Z","iopub.execute_input":"2025-01-23T05:03:52.356851Z","iopub.status.idle":"2025-01-23T05:07:45.061393Z","shell.execute_reply.started":"2025-01-23T05:03:52.356831Z","shell.execute_reply":"2025-01-23T05:07:45.060567Z"}},"outputs":[{"name":"stdout","text":"======== Epoch 1 / 3 ========\n","output_type":"stream"},{"name":"stderr","text":"Training Batch: 97it [01:06,  1.46it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nRunning Validation...\n  Validation Loss: 0.44\n  Accuracy: 0.81\n  F1 Macro: 0.78\n  F1 Micro: 0.81\n  F1 Weighted: 0.82\nValidation loss decreased (inf --> 0.44).  Saving model ...\n======== Epoch 2 / 3 ========\n","output_type":"stream"},{"name":"stderr","text":"Training Batch: 97it [01:13,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nRunning Validation...\n  Validation Loss: 0.44\n  Accuracy: 0.83\n  F1 Macro: 0.79\n  F1 Micro: 0.83\n  F1 Weighted: 0.83\n======== Epoch 3 / 3 ========\n","output_type":"stream"},{"name":"stderr","text":"Training Batch: 97it [01:13,  1.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nRunning Validation...\n  Validation Loss: 0.46\n  Accuracy: 0.83\n  F1 Macro: 0.79\n  F1 Micro: 0.83\n  F1 Weighted: 0.83\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"# 베스트 모델 로드\nmodel.load_state_dict(torch.load(\"model_checkpoint.pt\"))\n\navg_val_loss, eval_metrics = evaluate(model, test_dataloader, device)\nprint(\"  Test Loss: {0:.2f}\".format(avg_val_loss))\nprint(\"  Accuracy: {0:.2f}\".format(eval_metrics['accuracy']))\nprint(\"  F1 Macro: {0:.2f}\".format(eval_metrics['f1_macro']))\nprint(\"  F1 Micro: {0:.2f}\".format(eval_metrics['f1_micro']))\nprint(\"  F1 Weighted: {0:.2f}\".format(eval_metrics['f1_weighted']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:07:45.062324Z","iopub.execute_input":"2025-01-23T05:07:45.062656Z","iopub.status.idle":"2025-01-23T05:07:53.278705Z","shell.execute_reply.started":"2025-01-23T05:07:45.062621Z","shell.execute_reply":"2025-01-23T05:07:53.277996Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-38-c4493010d792>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(\"model_checkpoint.pt\"))\n","output_type":"stream"},{"name":"stdout","text":"  Test Loss: 0.44\n  Accuracy: 0.82\n  F1 Macro: 0.81\n  F1 Micro: 0.82\n  F1 Weighted: 0.82\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\"text-classification\", model=model.cuda(), tokenizer=tokenizer, device=0, max_length=512, return_all_scores=True, function_to_apply='softmax')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:07:53.279660Z","iopub.execute_input":"2025-01-23T05:07:53.279906Z","iopub.status.idle":"2025-01-23T05:07:53.542734Z","shell.execute_reply.started":"2025-01-23T05:07:53.279884Z","shell.execute_reply":"2025-01-23T05:07:53.541857Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"result = pipe('SK하이닉스가 매출이 급성장하였다')\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:07:53.543779Z","iopub.execute_input":"2025-01-23T05:07:53.544060Z","iopub.status.idle":"2025-01-23T05:07:53.580096Z","shell.execute_reply.started":"2025-01-23T05:07:53.544037Z","shell.execute_reply":"2025-01-23T05:07:53.579398Z"}},"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"[[{'label': 'LABEL_0', 'score': 0.14193099737167358}, {'label': 'LABEL_1', 'score': 0.8543503284454346}, {'label': 'LABEL_2', 'score': 0.003718655090779066}]]\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"# return_all_scores 제거\npipe = pipeline(\"text-classification\", model=model.cuda(), tokenizer=tokenizer, device=0, max_length=512, function_to_apply='softmax')\n\nresult = pipe('SK하이닉스가 매출이 급성장하였다')\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:07:53.580907Z","iopub.execute_input":"2025-01-23T05:07:53.581248Z","iopub.status.idle":"2025-01-23T05:07:53.602199Z","shell.execute_reply.started":"2025-01-23T05:07:53.581215Z","shell.execute_reply":"2025-01-23T05:07:53.601439Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"[{'label': 'LABEL_1', 'score': 0.8543503284454346}]\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"label_dict = {'LABEL_0' : '중립', 'LABEL_1' : '긍정', 'LABEL_2' : '부정'}\n\ndef prediction(text):\n  result = pipe(text)\n\n  return [label_dict[result[0]['label']]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:07:53.603062Z","iopub.execute_input":"2025-01-23T05:07:53.603338Z","iopub.status.idle":"2025-01-23T05:07:53.611120Z","shell.execute_reply.started":"2025-01-23T05:07:53.603317Z","shell.execute_reply":"2025-01-23T05:07:53.610303Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"prediction('네이버가 매출이 급성장하였다')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:07:53.611874Z","iopub.execute_input":"2025-01-23T05:07:53.612108Z","iopub.status.idle":"2025-01-23T05:07:53.653072Z","shell.execute_reply.started":"2025-01-23T05:07:53.612071Z","shell.execute_reply":"2025-01-23T05:07:53.652395Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"['긍정']"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"prediction('ChatGPT의 등장으로 인공지능 스타트업들은 위기다')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-23T05:07:53.653799Z","iopub.execute_input":"2025-01-23T05:07:53.654033Z","iopub.status.idle":"2025-01-23T05:07:53.668724Z","shell.execute_reply.started":"2025-01-23T05:07:53.654014Z","shell.execute_reply":"2025-01-23T05:07:53.667892Z"}},"outputs":[{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"['부정']"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}